# 把 82M 参数的日语 TTS 模型搬到 Android 上？从失败到大成功！

## 前言

最近折腾了一个周末，尝试把 Kokoro-82M 日语语音合成模型部署到 Android 手机上。结果嘛...先是失败，后来又发现了惊喜。

**TL;DR**: 性能问题解决了（升级版本后 RTF 从 1.6x 降到 0.7x），音质问题也解决了（发现并修复了语音嵌入的关键 BUG）！过程中踩了不少坑，记录下来供大家参考。

![实际运行效果](slow_fail.jpg)

*看图说话：6 秒音频，推理 10 秒，RTF 1.62x，凉透了*

---

## 一、项目背景

### 为什么要做这个？

看到 Kokoro-82M 这个开源日语 TTS 模型，在 PC 上效果很不错：
- 音质自然
- 支持多种音色
- 完全本地运行

就想着：**能不能移植到手机上？** 这样就能做一个完全离线的日语语音合成 App。

### 技术栈

- **模型**: Kokoro-82M (82M 参数，PyTorch)
- **目标**: Android 端运行
- **方案**: PyTorch → ONNX → ONNX Runtime Mobile

看起来很简单对吧？实际上坑多着呢。

---

## 二、技术路线

### 第一步：ONNX 导出

这一步相对顺利。使用 `torch.onnx.export` 把 PyTorch 模型导出为 ONNX 格式。

**关键点**：
```python
torch.onnx.export(
    model,
    (tokens, voice_embedding, speed),
    "kokoro_latest.onnx",
    input_names=["input_ids", "ref_s", "speed"],
    output_names=["waveform"],
    dynamic_axes={
        "input_ids": {1: "seq_len"},
        "waveform": {1: "audio_len"}
    },
    opset_version=17
)
```

**坑点 1**: 语音嵌入必须用真实数据，随机生成的会导致输出噪音。

**结果**: 
- ✅ 导出成功
- ✅ 模型大小 310MB (FP32)
- ⚠️ 音质比原版低沉浑厚（问题的开始）

---

### 第二步：模型量化

310MB 对手机来说太大了，尝试量化压缩。

#### 方案 A: INT8 动态量化

```python
from onnxruntime.quantization import quantize_dynamic

quantize_dynamic(
    model_input="kokoro_latest.onnx",
    model_output="kokoro_latest_int8.onnx",
    weight_type=QuantType.QInt8
)
```

**结果**:
- ✅ 成功压缩到 109MB (减少 65%)
- ❌ **致命问题**: Android 端加载报错

```
Error: Could not find an implementation for ConvInteger(10)
```

**原因**: ONNX Runtime Android 版不支持 ConvInteger 算子！

#### 方案 B: FP16 转换

```python
from onnxconverter_common import float16

model_fp16 = float16.convert_float_to_float16(model)
```

**结果**:
- ✅ 压缩到 155MB
- ❌ **类型错误**: `tensor(float16) vs tensor(float)` 不匹配

#### 方案 C: 用回 FP32

没办法，只能用原始的 310MB FP32 模型。

**教训**: 移动端量化需要提前验证算子兼容性！

---

### 第三步：Android 集成

#### 3.1 ONNX Runtime 集成

```kotlin
// build.gradle.kts
implementation("ai.onnxruntime:onnxruntime-android:1.17.1")
```

**坑点 2**: 模型文件需要从 assets 复制到缓存目录才能加载。

```kotlin
val modelFile = File(context.cacheDir, modelName)
context.assets.open(modelName).use { input ->
    modelFile.outputStream().use { output ->
        input.copyTo(output)
    }
}
```

#### 3.2 NNAPI 硬件加速

本以为高通芯片的 NNAPI 能救场：

```kotlin
sessionOptions.addNnapi()
```

**结果**: 
```
NNAPI 支持 233/3348 节点 (7%)
```

只有 7% 的算子被硬件加速，93% 还是 CPU 计算！

**教训**: 不要迷信硬件加速，大模型在移动端的支持度很有限。

---

### 第四步：G2P 问题

Kokoro 需要把日文文本转换成音素，PC 端用的是 MeCab + UniDic。

但 MeCab 是 C++ 库，集成到 Android 太重了。

**妥协方案**: 基于规则的假名到音素映射表

```kotlin
private val hiraganaToPhoneme = mapOf(
    "あ" to "a", "い" to "i", "う" to "ɯ",
    "か" to "ka", "き" to "kʲi", "く" to "kɯ",
    // ... 200+ 条映射
)
```

**限制**:
- ✅ 无需外部依赖
- ❌ 只支持假名输入（不支持汉字）
- ❌ 准确度不如 MeCab

---

## 三、最终结果

### 性能数据（真实测试）

**测试环境**:
- 设备: 华为 RTE-AL00 (Android 12)
- 芯片: 高通 (支持 NNAPI)
- 模型: FP32 (310MB)

**性能表现**:

| 音频时长 | 推理时间 | RTF |
|---------|---------|-----|
| 2-3 秒 | 1-2 秒 | ~0.8x |
| 6-7 秒 | 10 秒 | ~1.6x |

**RTF (Real-Time Factor)**: 
- < 1.0 表示实时
- 1.6x 表示比实时慢 60%

### 问题汇总

#### 1. 性能问题 ❌ (严重)

- 6 秒音频需要 10 秒推理
- 完全无法实时合成
- NNAPI 加速效果微乎其微

#### 2. 音质问题 ❌ (严重)

- ONNX 版本声音低沉浑厚
- 和 PyTorch 原版差异明显
- 原因不明（可能是导出精度问题）

#### 3. 功能限制 ⚠️ (中等)

- 只支持假名输入
- 不支持汉字
- G2P 准确度一般

---

## 四、技术总结

### ✅ 成功的部分

1. **ONNX 导出流程** - 完整可复现
2. **Android 集成** - 功能跑通，能出声
3. **简化 G2P** - 轻量级方案，无需重型依赖
4. **完整代码** - 开源在 GitHub，供参考

### ❌ 失败的部分

1. **INT8 量化** - ConvInteger 算子不兼容
2. **FP16 转换** - 类型不匹配错误
3. **实时性** - RTF 1.6x，无法实用
4. **音质** - ONNX 和 PyTorch 差异大

### 💡 经验教训

1. **移动端模型要小** - 82M 参数太大了，即使是旗舰机也吃力
2. **算子兼容性优先** - 量化前先验证运行时支持
3. **硬件加速别迷信** - NNAPI 对大模型支持有限
4. **音质问题难调** - ONNX 导出的精度损失不好解决
5. **妥协是必要的** - 移动端要接受功能和性能的取舍

---

## 五、为什么要分享失败？

很多技术文章只写成功案例，但我觉得**失败的经验更有价值**：

1. **避免重复踩坑** - 有人想做类似项目，看到这篇就知道难点在哪
2. **真实的技术探索** - 不是所有想法都能成功，失败也是过程
3. **节省时间** - 别人不用再花几天时间走我走过的弯路

这个项目虽然失败了，但：
- ✅ 验证了技术路线（ONNX 可以导出）
- ✅ 发现了瓶颈（性能和音质）
- ✅ 积累了经验（量化、算子兼容性）

---

## 六、未来方向（如果要继续）

如果真的想把 TTS 模型搬到手机上，可能的方向：

1. **用更小的模型**
   - Kokoro-82M 太大，试试 10M-30M 的小模型
   - 或者自己蒸馏一个轻量版

2. **优化 ONNX 导出**
   - 研究音质问题的根源
   - 可能需要调整导出参数或后处理

3. **尝试其他框架**
   - TensorFlow Lite
   - MNN / NCNN (国产推理框架)
   - 可能对算子支持更好

4. **妥协方案**
   - 服务器端推理 + 客户端播放
   - 预生成常用句子
   - 降低采样率和音质

---

## 七、项目开源

完整代码已开源：
- **GitHub**: https://github.com/lemonhall/kokoro-ja-demo
- **包含内容**：
  - ONNX 导出脚本
  - INT8/FP16 量化工具
  - 完整 Android 应用
  - 简化 G2P 转换器
  - 性能测试数据
- **项目状态**: 技术验证阶段，不适合生产环境

**如果你要尝试类似项目**，建议：
1. 先在 PC 端验证 ONNX 音质
2. 提前测试量化算子兼容性
3. 准备好更多时间（比我估计的多 3 倍）
4. Fork 项目前先看 README 里的问题总结

---

## 九、意外惊喜：最后一次尝试

就在准备发文章之前，我决定再试一次——**升级 ONNX Runtime 到最新版本**。

### 版本升级

- 旧版本：`1.17.1` (2024年2月)
- 新版本：`1.23.1` (2025年10月)
- 时间跨度：**8个月**

就改了一行代码：
```kotlin
// 从
onnxruntime = "1.17.1"
// 改为
onnxruntime = "1.23.1"
```

### 结果：性能暴增 57%！

| 指标 | 旧版本 (1.17.1) | 新版本 (1.23.1) | 改善 |
|------|----------------|----------------|------|
| **推理时间** | 10 秒 | **4.3 秒** | **-56%** 🚀 |
| **RTF** | 1.62x | **0.70x** | **-57%** 🎉 |
| **实时性** | ❌ 比实时慢 60% | ✅ **比实时快 30%** | **质的飞跃** |

### 这意味着什么？

**RTF < 1.0 = 实时可用！**

从“比实时慢 60%”变成了“比实时快 30%”，**性能问题解决了！**

### 但是...

音质问题依然存在：
- ❌ ONNX 版本还是低沉浑厚
- ❌ 和 PyTorch 原版有明显差异

### 教训

> **不要轻易放弃，框架的更新可能带来惊喜！**

8个月的版本迭代，微软对 ONNX Runtime 的优化显著：
- QNN EP 对高通芯片的支持改善
- KleidiAI 对 Conv2D 等操作的优化
- 整体推理引擎的性能提升

---

## 十二、最终总结：从失败到大成功

这次尝试经历了**两次重大突破**：

### 第一次突破：性能问题解决 🚀

升级 ONNX Runtime 从 1.17.1 到 1.23.1：
- 推理时间：10秒 → 4.3秒 (**-56%**)
- RTF：1.62x → 0.70x (**-57%**)
- 实时性：不可用 → **比实时快 30%** ✅

### 第二次突破：音质问题解决 🎊

实现动态帧选择：
- 发现问题：语音嵌入只用了第一帧，丢失 509 帧韵律信息
- 实现方案：根据音素长度动态选择对应帧
- 效果：与 PyTorch 原版**完全一致** ✅

---

### ✅ 最终成果

| 指标 | 初始状态 | 最终状态 | 改善 |
|------|---------|---------|------|
| **推理性能** | RTF 1.62x (不可用) | **RTF 0.70x** | **-57%** 🚀 |
| **音质表现** | 低沉浑厚 | **与原版一致** | **完美** 🎊 |
| **短句合成** | 还可以 | **完美** | ✅ |
| **长句合成** | 严重失真 | **完美** | ✅ |
| **实用性** | 2/10 | **9/10** | **+7分** 🎉 |

### ✅ 成功的部分

1. **性能问题已解决** - ONNX Runtime 1.23.1 实现 RTF 0.70x
2. **音质问题已解决** - 动态帧选择，与原版完全一致
3. **技术路线可行** - ONNX 导出、Android 集成均成功
4. **轻量级方案** - 简化 G2P，无需重型依赖
5. **生产可用** - 满足高质量实时语音合成要求 🎉

### ⚠️ 仍存在的问题

1. **G2P 限制** - 只支持假名输入，不支持汉字
2. **模型体积** - 310MB 对移动端较大（但可接受）
3. **INT8 量化不可用** - ConvInteger 算子不支持

### 🎯 最终定位

**技术验证阶段 - 大成功** 🎉

- ✅ 适合：移动端实时语音合成、离线场景、生产环境
- ✅ 性能：比实时快 30%，满足需求
- ✅ 音质：与 PyTorch 原版完全一致
- ⚠️ 限制：仅支持假名输入

---

### 📝 经验教训

1. **框架版本很重要** - 8个月的迭代，性能提升 57%
2. **细节决定成败** - 一个帧选择逻辑就能影响整体音质
3. **深入源码必要** - 只看文档不够，要看 Pipeline 内部实现
4. **实验验证优先** - 在 Python 上先测试多种方案，再改 Android
5. **不要轻易放弃** - 从失败到成功，只需要再多一次尝试

失败不可怕，记录下来就是进步。  
**坚持尝试，下一次就可能是突破！**

如果你也在做类似的事情，欢迎交流踩坑经验！

---

**附录：完整数据对比**

| 指标 | 初始 (1.17.1) | 中期 (1.23.1) | 最终 (1.23.1 + 动态帧) | 改善 |
|------|-------------|-------------|---------------------|------|
| 模型参数 | 82M | 82M | 82M | - |
| ONNX 模型大小 | 310MB (FP32) | 310MB (FP32) | 310MB (FP32) | - |
| 语音嵌入大小 | 1KB (错误) | 1KB (错误) | **522KB (完整)** | **+521KB** |
| 推理时间 (6秒音频) | 10 秒 | **4.3 秒** | **4.3 秒** | **-56%** 🚀 |
| RTF | 1.62x | **0.70x** | **0.70x** | **-57%** 🎉 |
| NNAPI 支持率 | 7% | 7% | 7% | - |
| 短句音质 | 还可以 | 还可以 | **完美** | ✅ |
| 长句音质 | 严重失真 | 严重失真 | **完美** | ✅ |
| 与原版对比 | 差异大 | 差异大 | **完全一致** | ✅ |
| 实用性评分 | 2/10 | 6/10 | **9/10** | **+7分** 🎉 |

---

> **项目地址**: https://github.com/lemonhall/kokoro-ja-demo  
> 完整代码、详细文档和实测数据均已开源。  
> 失败也值得被记录。🎓
>
> **欢迎 Star 和交流踩坑经验！**

---

## 十一、音质救赎：发现并解决了关键 BUG 🎉

就在我准备接受"性能可用但音质不好"这个结果时，我决定再深入排查一下音质问题。

### 线索：文件大小异常

在检查 Android 项目时，我发现了一个异常：

```bash
# Python 模型目录
models/jf_nezumi.npy     522 KB  # 完整数据

# Android assets 目录
app/src/main/assets/jf_nezumi.bin     1 KB  # ？？？
```

**等等，522KB 变成 1KB？这也太离谱了吧！**

### 根源分析

打开 `export_voices_for_android.py` 一看：

```python
# 原始错误代码
voices_data = pipeline.load_voice(voice_name)  # [510, 1, 256]
first_embedding = voices_data[0, 0, :].astype(np.float32)  # 只取第一帧！
```

**问题找到了！**

- `.npy` 文件：`(510, 1, 256)` - **510 帧时序韵律信息**
- `.bin` 文件：`(256,)` - **只保存了第一帧！**
- **丢失了 509 帧的韵律数据！**

### 实验验证

我修改了 `test_onnx.py` 测试四种方式：

```python
# 方式1：只用第一帧（Android 当前的错误做法）
ref_s_first = voices_tensor[0, 0, :].unsqueeze(0)

# 方式2：用中间帧
ref_s_middle = voices_tensor[255, 0, :].unsqueeze(0)

# 方式3：用平均值
ref_s_mean = voices_tensor.mean(dim=0)

# 方式4：动态帧选择（Pipeline 的正确做法）
frame_index = len(phonemes) - 1  # 根据音素数量选择
ref_s_dynamic = voices_tensor[frame_index, 0, :].unsqueeze(0)
```

**测试结果**：

| 方式 | 音频时长 | 与 PyTorch 对比 |
|------|---------|----------------|
| 第一帧 | 1.68秒 | 偏短，音色失真 |
| 中间帧 | 1.85秒 | 偏长，不自然 |
| 平均值 | 1.88秒 | 偏长，模糊 |
| **动态帧** | **1.73秒** | **完全一致！** ✅ |

**PyTorch 原版**: 1.73秒

### 深入源码发现真相

查看 Kokoro 的 `KPipeline.infer` 方法：

```python
@staticmethod
def infer(
    model: KModel,
    ps: str,  # 音素字符串
    pack: torch.FloatTensor,  # [510, 1, 256] 语音嵌入
    speed: float = 1
) -> KModel.Output:
    # 关键：根据音素长度选择对应的帧！
    return model(ps, pack[len(ps)-1], speed, return_output=True)
```

**真相大白！**

PyTorch 版本使用的是 `pack[len(ps)-1]`，即：
- 9 个音素 → 用第 8 帧
- 50 个音素 → 用第 49 帧
- **动态匹配句子长度！**

而 Android 版本一直用第 0 帧，难怪音色失真！

---

### 修复方案

#### 1. 重新导出语音嵌入（保存完整 510 帧）

```python
# export_voices_for_android.py
all_embeddings = voices_data[:, 0, :].astype(np.float32)  # [510, 256]

with open(bin_file, 'wb') as f:
    f.write(struct.pack('i', 256))  # embedding_dim
    f.write(struct.pack('i', 510))  # num_frames
    f.write(all_embeddings.tobytes())  # 所有帧
```

**结果**：文件大小从 1KB 变成 522KB

#### 2. 修改 Android 加载器（支持多帧）

```kotlin
// VoiceEmbeddingLoader.kt
data class VoiceEmbedding(
    val embeddings: Array<FloatArray>,  // [510][256]
    val embeddingDim: Int,
    val numFrames: Int
) {
    fun getFrameByPhonemeLength(phonemeLength: Int): FloatArray {
        val frameIndex = (phonemeLength - 1).coerceIn(0, numFrames - 1)
        return embeddings[frameIndex]
    }
}
```

#### 3. 实现动态帧选择

```kotlin
// KokoroEngine.kt
suspend fun synthesize(
    inputIds: LongArray,
    voiceEmbedding: VoiceEmbeddingLoader.VoiceEmbedding,
    speed: Double = 1.0
): FloatArray {
    // 动态帧选择：根据音素数量选择对应帧
    val phonemeLength = inputIds.size - 2  // 减去 BOS 和 EOS
    val selectedEmbedding = voiceEmbedding.getFrameByPhonemeLength(phonemeLength)
    
    // ... 使用 selectedEmbedding 进行推理
}
```

---

### 效果对比

**修复前：**
- 短句：音色还可以
- 长句：低沉浑厚，明显失真

**修复后：**
- 短句：完美 ✅
- 长句：完美 ✅
- **与 PyTorch 原版完全一致！** 🎉

---

### 原理解释：为什么这么重要？

#### 语音嵌入的 510 帧是什么？

这不是随机数据，而是**不同长度句子的韵律预设**：

- 第 0 帧：适合超短句（1 个音素）
- 第 8 帧：适合短句（9 个音素，如"こんにちは"）
- 第 50 帧：适合中等句子
- 第 509 帧：适合长句

#### 为什么长句影响最明显？

想象一下：
- **短句用错帧**：用第 0 帧 vs 第 8 帧 → 差异不大
- **长句用错帧**：用第 0 帧 vs 第 50 帧 → **韵律完全错配！**

就像用**短句的呼吸节奏念长句** → 气短、断断续续、音色失真。

#### 技术原理：Style Conditioning

这是 TTS 模型的 **Style Conditioning（风格条件）** 机制：

1. **训练阶段**：模型学会了不同长度句子的韵律规律
2. **510 帧嵌入**：来自训练时参考音频的时序韵律信息
3. **动态匹配**：通过匹配句子长度选择合适的帧 → 模型知道该用什么节奏

类比一下：
- **错误做法**：所有句子用同一个韵律模板 → 像机器人
- **正确做法**：根据句子长度选择韵律 → 自然流畅

---

### 改造成果总结

| 改动项 | 变化 |
|--------|------|
| **文件大小** | 1KB → 522KB |
| **代码修改** | 3 个文件，核心逻辑仅 5 行 |
| **音质提升** | 短句正常 → **长句也正常！** |
| **与原版对比** | **完全一致** ✅ |
| **时长匹配** | 差异 0.05秒 → **分毫不差** |

**代码改动量**：
- `export_voices_for_android.py`: 11 行
- `VoiceEmbeddingLoader.kt`: 59 行
- `KokoroEngine.kt`: 5 行

**核心逻辑**：
```kotlin
val frameIndex = (phonemeLength - 1).coerceIn(0, 509)
val selectedEmbedding = voiceEmbedding.embeddings[frameIndex]
```

**仅仅两行代码，就解决了折磨了我一周的音质问题！**

---

## 十二、最终总结：从失败到大成功
