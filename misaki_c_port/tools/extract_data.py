#!/usr/bin/env python3
"""
ä» Python misaki æå–æ•°æ®å¹¶è½¬æ¢ä¸º C è¯­è¨€å¯ç”¨çš„æ ¼å¼

ç›®æ ‡ï¼š
1. æå–æ‰€æœ‰è¯­è¨€çš„è¯å…¸æ•°æ®
2. åˆ†ææ•°æ®ç»“æ„
3. ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
4. è½¬æ¢ä¸ºç®€åŒ–çš„æ–‡æœ¬æ ¼å¼ï¼ˆç¬¬ä¸€é˜¶æ®µï¼‰
5. åç»­å¯è½¬æ¢ä¸ºäºŒè¿›åˆ¶æ ¼å¼ï¼ˆç¬¬äºŒé˜¶æ®µï¼‰

è¾“å‡ºç›®å½•ï¼šmisaki_c_port/extracted_data/
"""

import json
import os
import sys
from pathlib import Path
from collections import Counter

# è¾“å‡ºç›®å½•
OUTPUT_DIR = Path("misaki_c_port/extracted_data")
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

# æºæ•°æ®ç›®å½•
SOURCE_DIR = Path("misaki/misaki/data")

def analyze_file_structure(file_path):
    """åˆ†ææ–‡ä»¶ç»“æ„"""
    print(f"\n{'='*60}")
    print(f"åˆ†ææ–‡ä»¶: {file_path.name}")
    print(f"{'='*60}")
    
    file_size = file_path.stat().st_size / 1024  # KB
    print(f"æ–‡ä»¶å¤§å°: {file_size:.2f} KB")
    
    if file_path.suffix == '.json':
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        print(f"æ•°æ®ç±»å‹: {type(data).__name__}")
        
        if isinstance(data, dict):
            print(f"è¯æ¡æ•°é‡: {len(data)}")
            
            # åˆ†æé”®å€¼ç»“æ„
            sample_items = list(data.items())[:5]
            print(f"\nç¤ºä¾‹æ•°æ®:")
            for key, value in sample_items:
                key_str = key if len(key) <= 30 else key[:30] + "..."
                val_str = str(value) if len(str(value)) <= 50 else str(value)[:50] + "..."
                print(f"  {key_str:30} â†’ {val_str}")
            
            # ç»Ÿè®¡é”®é•¿åº¦
            key_lengths = [len(k) for k in data.keys()]
            print(f"\né”®é•¿åº¦ç»Ÿè®¡:")
            print(f"  æœ€å°: {min(key_lengths)}")
            print(f"  æœ€å¤§: {max(key_lengths)}")
            print(f"  å¹³å‡: {sum(key_lengths)/len(key_lengths):.2f}")
            
            # ç»Ÿè®¡å€¼é•¿åº¦
            if sample_items and isinstance(sample_items[0][1], str):
                val_lengths = [len(str(v)) for v in data.values()]
                print(f"\nå€¼é•¿åº¦ç»Ÿè®¡:")
                print(f"  æœ€å°: {min(val_lengths)}")
                print(f"  æœ€å¤§: {max(val_lengths)}")
                print(f"  å¹³å‡: {sum(val_lengths)/len(val_lengths):.2f}")
        
        return data
    
    elif file_path.suffix == '.txt':
        with open(file_path, 'r', encoding='utf-8') as f:
            lines = [line.strip() for line in f if line.strip()]
        
        print(f"è¡Œæ•°: {len(lines)}")
        print(f"\nå‰ 10 è¡Œ:")
        for line in lines[:10]:
            print(f"  {line}")
        
        # ç»Ÿè®¡é•¿åº¦
        lengths = [len(line) for line in lines]
        print(f"\nè¡Œé•¿åº¦ç»Ÿè®¡:")
        print(f"  æœ€å°: {min(lengths)}")
        print(f"  æœ€å¤§: {max(lengths)}")
        print(f"  å¹³å‡: {sum(lengths)/len(lengths):.2f}")
        
        return lines
    
    return None

def extract_english_dict():
    """æå–è‹±æ–‡è¯å…¸ (CMUdict)"""
    print(f"\n{'#'*60}")
    print(f"# æå–è‹±æ–‡è¯å…¸")
    print(f"{'#'*60}")
    
    output_dir = OUTPUT_DIR / "en"
    output_dir.mkdir(exist_ok=True)
    
    # ç¾å¼è‹±è¯­
    us_gold = analyze_file_structure(SOURCE_DIR / "us_gold.json")
    us_silver = analyze_file_structure(SOURCE_DIR / "us_silver.json")
    
    # è‹±å¼è‹±è¯­
    gb_gold = analyze_file_structure(SOURCE_DIR / "gb_gold.json")
    gb_silver = analyze_file_structure(SOURCE_DIR / "gb_silver.json")
    
    # åˆå¹¶è¯å…¸ï¼ˆä¼˜å…ˆ goldï¼‰
    us_dict = {**us_silver, **us_gold}
    gb_dict = {**gb_silver, **gb_gold}
    
    print(f"\nåˆå¹¶å:")
    print(f"  ç¾å¼è‹±è¯­: {len(us_dict)} è¯æ¡")
    print(f"  è‹±å¼è‹±è¯­: {len(gb_dict)} è¯æ¡")
    
    # å¯¼å‡ºä¸ºæ–‡æœ¬æ ¼å¼ï¼ˆæ˜“äº C è¯­è¨€è§£æï¼‰
    with open(output_dir / "us_dict.txt", 'w', encoding='utf-8') as f:
        for word, ipa in sorted(us_dict.items()):
            f.write(f"{word}\t{ipa}\n")
    
    with open(output_dir / "gb_dict.txt", 'w', encoding='utf-8') as f:
        for word, ipa in sorted(gb_dict.items()):
            f.write(f"{word}\t{ipa}\n")
    
    print(f"\nâœ… å·²å¯¼å‡º:")
    print(f"  {output_dir / 'us_dict.txt'}")
    print(f"  {output_dir / 'gb_dict.txt'}")
    
    # ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯
    stats = {
        'us_total': len(us_dict),
        'gb_total': len(gb_dict),
        'us_unique': len(set(us_dict.keys()) - set(gb_dict.keys())),
        'gb_unique': len(set(gb_dict.keys()) - set(us_dict.keys())),
        'common': len(set(us_dict.keys()) & set(gb_dict.keys())),
    }
    
    with open(output_dir / "stats.json", 'w', encoding='utf-8') as f:
        json.dump(stats, f, indent=2, ensure_ascii=False)
    
    return us_dict, gb_dict

def extract_japanese_dict():
    """æå–æ—¥æ–‡è¯å…¸"""
    print(f"\n{'#'*60}")
    print(f"# æå–æ—¥æ–‡è¯å…¸")
    print(f"{'#'*60}")
    
    output_dir = OUTPUT_DIR / "ja"
    output_dir.mkdir(exist_ok=True)
    
    words = analyze_file_structure(SOURCE_DIR / "ja_words.txt")
    
    # ç›´æ¥å¤åˆ¶ï¼ˆå·²ç»æ˜¯æ–‡æœ¬æ ¼å¼ï¼‰
    with open(output_dir / "words.txt", 'w', encoding='utf-8') as f:
        for word in words:
            f.write(f"{word}\n")
    
    print(f"\nâœ… å·²å¯¼å‡º:")
    print(f"  {output_dir / 'words.txt'}")
    
    # ç»Ÿè®¡ä¿¡æ¯
    stats = {
        'total': len(words),
        'unique': len(set(words)),
    }
    
    with open(output_dir / "stats.json", 'w', encoding='utf-8') as f:
        json.dump(stats, f, indent=2, ensure_ascii=False)
    
    return words

def extract_vietnamese_dict():
    """æå–è¶Šå—æ–‡è¯å…¸"""
    print(f"\n{'#'*60}")
    print(f"# æå–è¶Šå—æ–‡è¯å…¸")
    print(f"{'#'*60}")
    
    output_dir = OUTPUT_DIR / "vi"
    output_dir.mkdir(exist_ok=True)
    
    acronyms = analyze_file_structure(SOURCE_DIR / "vi_acronyms.json")
    symbols = analyze_file_structure(SOURCE_DIR / "vi_symbols.json")
    teencode = analyze_file_structure(SOURCE_DIR / "vi_teencode.json")
    
    # å¯¼å‡º
    with open(output_dir / "acronyms.txt", 'w', encoding='utf-8') as f:
        for word, replacement in sorted(acronyms.items()):
            f.write(f"{word}\t{replacement}\n")
    
    with open(output_dir / "symbols.txt", 'w', encoding='utf-8') as f:
        for symbol, replacement in sorted(symbols.items()):
            f.write(f"{symbol}\t{replacement}\n")
    
    with open(output_dir / "teencode.txt", 'w', encoding='utf-8') as f:
        for code, replacement in sorted(teencode.items()):
            f.write(f"{code}\t{replacement}\n")
    
    print(f"\nâœ… å·²å¯¼å‡º:")
    print(f"  {output_dir / 'acronyms.txt'}")
    print(f"  {output_dir / 'symbols.txt'}")
    print(f"  {output_dir / 'teencode.txt'}")
    
    return acronyms, symbols, teencode

def extract_chinese_data():
    """æå–ä¸­æ–‡æ•°æ®ï¼ˆä» pypinyinï¼‰"""
    print(f"\n{'#'*60}")
    print(f"# æå–ä¸­æ–‡æ•°æ®")
    print(f"{'#'*60}")
    
    output_dir = OUTPUT_DIR / "zh"
    output_dir.mkdir(exist_ok=True)
    
    try:
        from pypinyin import pinyin_dict
        
        print(f"pypinyin è¯å…¸å¤§å°: {len(pinyin_dict.pinyin_dict)}")
        
        # å¯¼å‡ºæ‹¼éŸ³è¯å…¸
        with open(output_dir / "pinyin_dict.txt", 'w', encoding='utf-8') as f:
            for char_code, pinyin_str in sorted(pinyin_dict.pinyin_dict.items()):
                char = chr(char_code)
                f.write(f"{char}\t{pinyin_str}\n")
        
        print(f"\nâœ… å·²å¯¼å‡º:")
        print(f"  {output_dir / 'pinyin_dict.txt'}")
        
        stats = {
            'total': len(pinyin_dict.pinyin_dict),
        }
        
        with open(output_dir / "stats.json", 'w', encoding='utf-8') as f:
            json.dump(stats, f, indent=2, ensure_ascii=False)
        
    except ImportError:
        print("âš ï¸  pypinyin æœªå®‰è£…ï¼Œè·³è¿‡ä¸­æ–‡æ•°æ®æå–")
        print("   æç¤º: è¿è¡Œ `uv pip install pypinyin` å®‰è£…")

def generate_summary_report():
    """ç”Ÿæˆæ€»ç»“æŠ¥å‘Š"""
    print(f"\n{'#'*60}")
    print(f"# ç”Ÿæˆæ€»ç»“æŠ¥å‘Š")
    print(f"{'#'*60}")
    
    report_path = OUTPUT_DIR / "EXTRACTION_REPORT.md"
    
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("# Misaki æ•°æ®æå–æŠ¥å‘Š\n\n")
        f.write(f"**æå–æ—¶é—´**: {__import__('datetime').datetime.now()}\n\n")
        
        f.write("## ğŸ“Š æ•°æ®ç»Ÿè®¡\n\n")
        
        # è‹±æ–‡
        if (OUTPUT_DIR / "en" / "stats.json").exists():
            with open(OUTPUT_DIR / "en" / "stats.json", 'r') as sf:
                en_stats = json.load(sf)
            f.write("### è‹±æ–‡è¯å…¸\n\n")
            f.write(f"- ç¾å¼è‹±è¯­: {en_stats['us_total']:,} è¯æ¡\n")
            f.write(f"- è‹±å¼è‹±è¯­: {en_stats['gb_total']:,} è¯æ¡\n")
            f.write(f"- å…±åŒè¯æ±‡: {en_stats['common']:,} è¯æ¡\n")
            f.write(f"- ç¾å¼ç‹¬æœ‰: {en_stats['us_unique']:,} è¯æ¡\n")
            f.write(f"- è‹±å¼ç‹¬æœ‰: {en_stats['gb_unique']:,} è¯æ¡\n\n")
        
        # æ—¥æ–‡
        if (OUTPUT_DIR / "ja" / "stats.json").exists():
            with open(OUTPUT_DIR / "ja" / "stats.json", 'r') as sf:
                ja_stats = json.load(sf)
            f.write("### æ—¥æ–‡è¯å…¸\n\n")
            f.write(f"- æ€»è¯æ¡: {ja_stats['total']:,}\n")
            f.write(f"- å»é‡å: {ja_stats['unique']:,}\n\n")
        
        # ä¸­æ–‡
        if (OUTPUT_DIR / "zh" / "stats.json").exists():
            with open(OUTPUT_DIR / "zh" / "stats.json", 'r') as sf:
                zh_stats = json.load(sf)
            f.write("### ä¸­æ–‡è¯å…¸\n\n")
            f.write(f"- æ‹¼éŸ³è¡¨: {zh_stats['total']:,} æ±‰å­—\n\n")
        
        f.write("## ğŸ“ æ–‡ä»¶ç»“æ„\n\n")
        f.write("```\n")
        f.write("extracted_data/\n")
        for lang_dir in sorted(OUTPUT_DIR.iterdir()):
            if lang_dir.is_dir():
                f.write(f"â”œâ”€â”€ {lang_dir.name}/\n")
                for file in sorted(lang_dir.iterdir()):
                    size = file.stat().st_size / 1024
                    f.write(f"â”‚   â”œâ”€â”€ {file.name} ({size:.1f} KB)\n")
        f.write("```\n\n")
        
        f.write("## ğŸ”§ æ•°æ®æ ¼å¼\n\n")
        f.write("æ‰€æœ‰è¯å…¸æ–‡ä»¶éƒ½é‡‡ç”¨ **åˆ¶è¡¨ç¬¦åˆ†éš”** (TSV) æ ¼å¼ï¼š\n\n")
        f.write("```\n")
        f.write("word<TAB>phonemes\n")
        f.write("```\n\n")
        f.write("**ç¤ºä¾‹**:\n")
        f.write("```\n")
        f.write("apple\tÃ¦pÉ™l\n")
        f.write("hello\thÉ™lËˆoÊŠ\n")
        f.write("```\n\n")
        
        f.write("## âœ… ä¸‹ä¸€æ­¥\n\n")
        f.write("1. åˆ†ææ•°æ®ç»“æ„ï¼Œè®¾è®¡ C è¯­è¨€æ•°æ®ç±»å‹\n")
        f.write("2. å®ç°æ–‡æœ¬æ ¼å¼åŠ è½½å™¨ï¼ˆTSV parserï¼‰\n")
        f.write("3. åç»­è½¬æ¢ä¸ºäºŒè¿›åˆ¶æ ¼å¼ï¼ˆå¯é€‰ï¼‰\n")
    
    print(f"\nâœ… æŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")

def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("Misaki æ•°æ®æå–å·¥å…·")
    print("=" * 60)
    
    # æ£€æŸ¥æºç›®å½•
    if not SOURCE_DIR.exists():
        print(f"âŒ é”™è¯¯: æºç›®å½•ä¸å­˜åœ¨: {SOURCE_DIR}")
        print("   è¯·ç¡®ä¿å·²å…‹éš† misaki ä»“åº“")
        sys.exit(1)
    
    # æå–å„è¯­è¨€æ•°æ®
    extract_english_dict()
    extract_japanese_dict()
    extract_vietnamese_dict()
    extract_chinese_data()
    
    # ç”ŸæˆæŠ¥å‘Š
    generate_summary_report()
    
    print(f"\n{'='*60}")
    print("âœ… æ•°æ®æå–å®Œæˆï¼")
    print(f"{'='*60}")
    print(f"\nè¾“å‡ºç›®å½•: {OUTPUT_DIR.absolute()}")
    print(f"æŸ¥çœ‹æŠ¥å‘Š: {OUTPUT_DIR / 'EXTRACTION_REPORT.md'}")

if __name__ == "__main__":
    main()
