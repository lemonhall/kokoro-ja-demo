/**
 * test_tokenizer_en.c
 * 
 * æµ‹è¯•è‹±æ–‡åˆ†è¯å™¨
 * 
 * License: MIT
 */

#include "misaki_tokenizer.h"
#include <stdio.h>
#include <string.h>
#include <assert.h>

// æµ‹è¯•ç»“æœç»Ÿè®¡
static int test_passed = 0;
static int test_failed = 0;

#define TEST_ASSERT(cond, msg) do { \
    if (!(cond)) { \
        printf("  âŒ FAIL: %s\n", msg); \
        test_failed++; \
        return; \
    } \
} while(0)

#define RUN_TEST(test_func) do { \
    printf("\nğŸ§ª Running %s...\n", #test_func); \
    test_func(); \
    test_passed++; \
} while(0)

/* ============================================================================
 * è‹±æ–‡åˆ†è¯å™¨æµ‹è¯•
 * ========================================================================== */

void test_en_tokenizer_simple() {
    // è‹±æ–‡åˆ†è¯å™¨ä¸éœ€è¦ createï¼Œç›´æ¥è°ƒç”¨å‡½æ•°
    const char *text = "Hello world";
    MisakiTokenList *tokens = misaki_en_tokenize(text);
    TEST_ASSERT(tokens != NULL, "åˆ†è¯ç»“æœä¸åº”ä¸º NULL");
    
    misaki_token_list_free(tokens);
    
    printf("  âœ… è‹±æ–‡åˆ†è¯å™¨è°ƒç”¨æˆåŠŸ\n");
}

void test_en_tokenize_simple() {
    // æµ‹è¯•ç®€å•å¥å­
    const char *text = "Hello world";
    MisakiTokenList *tokens = misaki_en_tokenize(text);
    TEST_ASSERT(tokens != NULL, "åˆ†è¯ç»“æœä¸åº”ä¸º NULL");
    
    int count = misaki_token_list_size(tokens);
    printf("  åˆ†è¯ç»“æœ: %d ä¸ªè¯\n", count);
    
    for (int i = 0; i < count; i++) {
        MisakiToken *token = misaki_token_list_get(tokens, i);
        printf("    [%d] '%s' (tag: %s)\n", i, token->text, token->tag ? token->tag : "NULL");
    }
    
    TEST_ASSERT(count >= 2, "è‡³å°‘åº”è¯¥åˆ†æˆ 2 ä¸ªè¯");
    
    misaki_token_list_free(tokens);
    
    printf("  âœ… è‹±æ–‡ç®€å•åˆ†è¯æµ‹è¯•é€šè¿‡\n");
}

void test_en_tokenize_punctuation() {
    // æµ‹è¯•æ ‡ç‚¹ç¬¦å·å¤„ç†
    const char *text = "Hello, world! How are you?";
    MisakiTokenList *tokens = misaki_en_tokenize(text);
    TEST_ASSERT(tokens != NULL, "åˆ†è¯ç»“æœä¸åº”ä¸º NULL");
    
    printf("  åˆ†è¯ç»“æœ: %d ä¸ªè¯\n", tokens->count);
    for (int i = 0; i < tokens->count; i++) {
        MisakiToken *token = misaki_token_list_get(tokens, i);
        printf("    [%d] '%s'\n", i, token->text);
    }
    
    TEST_ASSERT(tokens->count > 0, "åº”è¯¥æœ‰åˆ†è¯ç»“æœ");
    
    misaki_token_list_free(tokens);
    
    printf("  âœ… è‹±æ–‡æ ‡ç‚¹ç¬¦å·æµ‹è¯•é€šè¿‡\n");
}

void test_en_tokenize_numbers() {
    // æµ‹è¯•æ•°å­—å¤„ç†
    const char *text = "I have 3 apples and 2.5 oranges.";
    MisakiTokenList *tokens = misaki_en_tokenize(text);
    TEST_ASSERT(tokens != NULL, "åˆ†è¯ç»“æœä¸åº”ä¸º NULL");
    
    printf("  åˆ†è¯ç»“æœ: %d ä¸ªè¯\n", tokens->count);
    for (int i = 0; i < tokens->count; i++) {
        MisakiToken *token = misaki_token_list_get(tokens, i);
        printf("    [%d] '%s'\n", i, token->text);
    }
    
    TEST_ASSERT(tokens->count > 0, "åº”è¯¥æœ‰åˆ†è¯ç»“æœ");
    
    misaki_token_list_free(tokens);
    
    printf("  âœ… è‹±æ–‡æ•°å­—å¤„ç†æµ‹è¯•é€šè¿‡\n");
}

void test_en_tokenize_contractions() {
    // æµ‹è¯•ç¼©å†™è¯å¤„ç†
    const char *text = "I'm don't can't won't";
    MisakiTokenList *tokens = misaki_en_tokenize(text);
    TEST_ASSERT(tokens != NULL, "åˆ†è¯ç»“æœä¸åº”ä¸º NULL");
    
    printf("  åˆ†è¯ç»“æœ: %d ä¸ªè¯\n", tokens->count);
    for (int i = 0; i < tokens->count; i++) {
        MisakiToken *token = misaki_token_list_get(tokens, i);
        printf("    [%d] '%s'\n", i, token->text);
    }
    
    misaki_token_list_free(tokens);
    
    printf("  âœ… è‹±æ–‡ç¼©å†™è¯æµ‹è¯•é€šè¿‡\n");
}

/* ============================================================================
 * ä¸»æµ‹è¯•å‡½æ•°
 * ========================================================================== */

int main(void) {
    printf("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n");
    printf("  è‹±æ–‡åˆ†è¯å™¨æµ‹è¯•\n");
    printf("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n");
    
    RUN_TEST(test_en_tokenizer_simple);
    RUN_TEST(test_en_tokenize_simple);
    RUN_TEST(test_en_tokenize_punctuation);
    RUN_TEST(test_en_tokenize_numbers);
    RUN_TEST(test_en_tokenize_contractions);
    
    // æ€»ç»“
    printf("\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n");
    printf("  æµ‹è¯•ç»“æœ:\n");
    printf("  âœ… é€šè¿‡: %d\n", test_passed);
    printf("  âŒ å¤±è´¥: %d\n", test_failed);
    printf("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n");
    
    return test_failed > 0 ? 1 : 0;
}
