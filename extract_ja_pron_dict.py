#!/usr/bin/env python3
"""
ä» MeCab/UniDic æå–æ—¥æ–‡è¯æ±‡+è¯»éŸ³è¯å…¸

ç”Ÿæˆæ ¼å¼ï¼šTSV (è¯æ±‡\tè¯»éŸ³\tè¯é¢‘\tè¯æ€§)
ä¾‹å¦‚ï¼š
ã“ã‚“ã«ã¡ã¯	ã‚³ãƒ³ãƒ‹ãƒãƒ¯	1000	æ„Ÿå‹•è©
ç§	ãƒ¯ã‚¿ã‚·	5000	ä»£åè©
å­¦ç”Ÿ	ã‚¬ã‚¯ã‚»ã‚¤	3000	åè©
"""

import sys

try:
    from fugashi import Tagger
except ImportError:
    print("âŒ é”™è¯¯ï¼šéœ€è¦å®‰è£… fugashi")
    print("è¿è¡Œï¼špip install fugashi unidic-lite")
    sys.exit(1)

def extract_dictionary():
    """ä» MeCab æå–è¯å…¸æ•°æ®"""
    print("ğŸ“– æ­£åœ¨åˆå§‹åŒ– MeCab...")
    tagger = Tagger()
    
    # æµ‹è¯•ç”¨çš„å¸¸ç”¨è¯æ±‡åˆ—è¡¨
    test_words = [
        # åŸºç¡€é—®å€™
        "ã“ã‚“ã«ã¡ã¯", "ã•ã‚ˆã†ãªã‚‰", "ãŠã¯ã‚ˆã†", "ã‚ã‚ŠãŒã¨ã†",
        # ä»£è¯
        "ç§", "ã‚ãªãŸ", "å½¼", "å½¼å¥³",
        # åŠ©è¯
        "ã¯", "ãŒ", "ã‚’", "ã«", "ã§", "ã¨", "ã®",
        # åè¯
        "å­¦ç”Ÿ", "å…ˆç”Ÿ", "å­¦æ ¡", "ä¼šç¤¾", "æ—¥æœ¬", "æ±äº¬",
        # åŠ¨è¯
        "è¡Œã", "æ¥ã‚‹", "é£Ÿã¹ã‚‹", "é£²ã‚€", "è¦‹ã‚‹", "èª­ã‚€", "æ›¸ã",
        # å½¢å®¹è¯
        "å¤§ãã„", "å°ã•ã„", "ç¾ã—ã„", "å…ƒæ°—",
        # ç‰‡å‡å
        "ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿", "ãƒ†ã‚¹ãƒˆ", "ãƒ—ãƒ­ã‚°ãƒ©ãƒ ",
        # è¾…åŠ©åŠ¨è¯
        "ã§ã™", "ã¾ã™", "ã¾ã—ãŸ", "ã§ã™ã‹",
    ]
    
    word_dict = {}
    
    print(f"\nğŸ“ å¤„ç† {len(test_words)} ä¸ªæµ‹è¯•è¯æ±‡...")
    
    for word in test_words:
        result = tagger(word)
        if result:
            for token in result:
                surface = token.surface
                feature = token.feature
                
                # è·å–è¯»éŸ³
                pron = feature.pron if hasattr(feature, 'pron') and feature.pron else None
                kana = feature.kana if hasattr(feature, 'kana') and feature.kana else None
                reading = pron or kana or surface
                
                # è·å–è¯æ€§
                pos = feature.pos1 if hasattr(feature, 'pos1') else "Unknown"
                
                # ç®€å•çš„è¯é¢‘ä¼°è®¡ï¼ˆåŸºäºè¯æ€§ï¼‰
                freq = 10000
                if pos == "åŠ©è©":
                    freq = 15000
                elif pos == "åŠ©å‹•è©":
                    freq = 12000
                elif pos == "ä»£åè©":
                    freq = 10000
                elif pos == "åè©":
                    freq = 8000
                elif pos == "å‹•è©":
                    freq = 7000
                    
                word_dict[surface] = (reading, freq, pos)
                
                print(f"  {surface:15s} â†’ {reading:15s} ({pos})")
    
    return word_dict

def save_dictionary(word_dict, output_file):
    """ä¿å­˜è¯å…¸åˆ°æ–‡ä»¶"""
    print(f"\nğŸ’¾ ä¿å­˜è¯å…¸åˆ°: {output_file}")
    
    with open(output_file, 'w', encoding='utf-8') as f:
        # å†™å…¥è¡¨å¤´
        f.write("# æ—¥æ–‡è¯æ±‡è¯»éŸ³è¯å…¸\n")
        f.write("# æ ¼å¼ï¼šè¯æ±‡<TAB>è¯»éŸ³<TAB>è¯é¢‘<TAB>è¯æ€§\n")
        f.write("#\n")
        
        # æŒ‰è¯é¢‘æ’åº
        sorted_words = sorted(word_dict.items(), key=lambda x: x[1][1], reverse=True)
        
        for word, (reading, freq, pos) in sorted_words:
            f.write(f"{word}\t{reading}\t{freq}\t{pos}\n")
    
    print(f"âœ… æˆåŠŸä¿å­˜ {len(word_dict)} ä¸ªè¯æ±‡")

def main():
    output_file = "extracted_data/ja/pron_dict.txt"
    
    print("="*60)
    print("  ä» MeCab/UniDic æå–æ—¥æ–‡è¯»éŸ³è¯å…¸")
    print("="*60)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    import os
    os.makedirs(os.path.dirname(output_file), exist_ok=True)
    
    # æå–è¯å…¸
    word_dict = extract_dictionary()
    
    # ä¿å­˜
    save_dictionary(word_dict, output_file)
    
    print("\nâœ¨ å®Œæˆï¼")
    print(f"\næç¤ºï¼šC ä»£ç å¯ä»¥ç›´æ¥åŠ è½½ {output_file}")
    print("      æ ¼å¼ï¼šword\\tpronunciation\\tfrequency\\tpos")

if __name__ == "__main__":
    main()
