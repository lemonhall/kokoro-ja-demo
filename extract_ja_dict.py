#!/usr/bin/env python3
"""
extract_ja_dict.py

ä» UniDic è¯å…¸æå–æ—¥æ–‡è¯æ±‡+è¯»éŸ³æ˜ å°„è¡¨

ç”Ÿæˆæ ¼å¼: TSV (è¯æ±‡<TAB>è¯»éŸ³<TAB>è¯é¢‘<TAB>è¯æ€§)

License: MIT
"""

import sys
import os
from pathlib import Path

try:
    from fugashi import Tagger
except ImportError:
    print("âŒ é”™è¯¯ï¼šéœ€è¦å®‰è£… fugashi")
    print("è¿è¡Œï¼šuv sync")
    sys.exit(1)

def load_word_list(file_path, limit=None):
    """ä»æ–‡ä»¶åŠ è½½è¯æ±‡åˆ—è¡¨"""
    words = []
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            word = line.strip()
            if word and not word.startswith('#'):
                words.append(word)
                if limit and len(words) >= limit:
                    break
    return words

def extract_pronunciations(tagger, words, verbose=True):
    """
    ä» MeCab æå–è¯æ±‡çš„è¯»éŸ³ä¿¡æ¯
    
    è¿”å›: dict {word: (pron, kana, pos, freq_estimate)}
    """
    word_dict = {}
    failed = 0
    
    for i, word in enumerate(words):
        if verbose and (i + 1) % 1000 == 0:
            print(f"  å¤„ç†è¿›åº¦: {i+1}/{len(words)} ({100*(i+1)/len(words):.1f}%)")
        
        try:
            result = tagger(word)
            if not result:
                failed += 1
                continue
            
            for token in result:
                surface = token.surface
                feature = token.feature
                
                # è·å–è¯»éŸ³ï¼ˆä¼˜å…ˆä½¿ç”¨ pronï¼Œå…¶æ¬¡ kanaï¼‰
                pron = getattr(feature, 'pron', None)
                kana = getattr(feature, 'kana', None)
                reading = pron or kana or surface
                
                # è·å–è¯æ€§
                pos1 = getattr(feature, 'pos1', 'Unknown')
                
                # ä¼°è®¡è¯é¢‘ï¼ˆåŸºäºè¯æ€§ï¼‰
                freq_map = {
                    "åŠ©è©": 15000,
                    "åŠ©å‹•è©": 12000,
                    "ä»£åè©": 10000,
                    "åè©": 8000,
                    "å‹•è©": 7000,
                    "å½¢å®¹è©": 6000,
                    "å½¢å®¹å‹•è©": 6000,
                    "å‰¯è©": 5500,
                    "æ„Ÿå‹•è©": 5000,
                    "æ¥ç¶šè©": 5000,
                    "é€£ä½“è©": 4500,
                    "æ¥é ­è¾": 4000,
                    "æ¥å°¾è¾": 4000,
                }
                freq = freq_map.get(pos1, 3000)
                
                # åŸºäºè¯é•¿è°ƒæ•´ï¼ˆçŸ­è¯é€šå¸¸æ›´å¸¸ç”¨ï¼‰
                freq = freq * (1.0 + 1.0 / max(len(surface), 1))
                
                # ä¿å­˜ï¼ˆå¦‚æœåŒä¸€è¯æœ‰å¤šä¸ªtokenï¼Œä¿ç•™ç¬¬ä¸€ä¸ªï¼‰
                if surface not in word_dict:
                    word_dict[surface] = (reading, kana or reading, pos1, int(freq))
                    
        except Exception as e:
            if verbose:
                print(f"  âš ï¸  å¤„ç†å¤±è´¥: {word} - {e}")
            failed += 1
    
    if verbose:
        print(f"\nâœ… æå–å®Œæˆ:")
        print(f"  - æˆåŠŸ: {len(word_dict)} ä¸ªè¯æ±‡")
        print(f"  - å¤±è´¥: {failed} ä¸ªè¯æ±‡")
    
    return word_dict

def save_dictionary(word_dict, output_file, verbose=True):
    """ä¿å­˜è¯å…¸åˆ° TSV æ–‡ä»¶"""
    output_dir = os.path.dirname(output_file)
    if output_dir:
        os.makedirs(output_dir, exist_ok=True)
    
    # æŒ‰è¯é¢‘æ’åº
    sorted_words = sorted(word_dict.items(), key=lambda x: x[1][3], reverse=True)
    
    with open(output_file, 'w', encoding='utf-8') as f:
        # å†™å…¥è¡¨å¤´
        f.write("# æ—¥æ–‡è¯æ±‡è¯»éŸ³è¯å…¸\n")
        f.write(f"# ç”Ÿæˆæ—¶é—´: {__import__('datetime').datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"# è¯æ¡æ•°: {len(word_dict)}\n")
        f.write("# æ ¼å¼: è¯æ±‡<TAB>è¯»éŸ³<TAB>è¯é¢‘<TAB>è¯æ€§\n")
        f.write("#\n")
        
        # å†™å…¥æ•°æ®
        for word, (pron, kana, pos, freq) in sorted_words:
            f.write(f"{word}\t{pron}\t{freq}\t{pos}\n")
    
    if verbose:
        print(f"\nğŸ’¾ è¯å…¸å·²ä¿å­˜: {output_file}")
        print(f"ğŸ“Š è¯æ¡æ•°: {len(word_dict)}")
        
        # ç»Ÿè®¡
        file_size = os.path.getsize(output_file)
        print(f"ğŸ“Š æ–‡ä»¶å¤§å°: {file_size/1024/1024:.2f} MB")
        
        # è¯æ€§åˆ†å¸ƒ
        pos_count = {}
        for _, (_, _, pos, _) in word_dict.items():
            pos_count[pos] = pos_count.get(pos, 0) + 1
        
        print(f"\nğŸ“Š è¯æ€§åˆ†å¸ƒï¼ˆå‰10ï¼‰:")
        for pos, count in sorted(pos_count.items(), key=lambda x: x[1], reverse=True)[:10]:
            print(f"  {pos:15s}: {count:6d} ({100*count/len(word_dict):5.1f}%)")

def main():
    print("="*70)
    print("  ä» UniDic æå–æ—¥æ–‡è¯æ±‡è¯»éŸ³è¯å…¸")
    print("="*70)
    
    # é…ç½®
    word_list_file = "misaki/misaki/data/ja_words.txt"
    output_file = "extracted_data/ja/ja_pron_dict.tsv"
    word_limit = 50000  # æå–å‰ 5ä¸‡ä¸ªè¯
    
    # æ£€æŸ¥è¯æ±‡åˆ—è¡¨æ–‡ä»¶
    if not os.path.exists(word_list_file):
        print(f"âŒ é”™è¯¯ï¼šè¯æ±‡åˆ—è¡¨æ–‡ä»¶ä¸å­˜åœ¨: {word_list_file}")
        return 1
    
    # åˆå§‹åŒ– MeCab
    print(f"\nğŸ“– æ­£åœ¨åˆå§‹åŒ– MeCab...")
    try:
        tagger = Tagger()
        print("âœ… MeCab åˆå§‹åŒ–æˆåŠŸ")
    except Exception as e:
        print(f"âŒ MeCab åˆå§‹åŒ–å¤±è´¥: {e}")
        return 1
    
    # åŠ è½½è¯æ±‡åˆ—è¡¨
    print(f"\nğŸ“š æ­£åœ¨åŠ è½½è¯æ±‡åˆ—è¡¨: {word_list_file}")
    words = load_word_list(word_list_file, limit=word_limit)
    print(f"âœ… å·²åŠ è½½ {len(words)} ä¸ªè¯æ±‡")
    
    # æå–è¯»éŸ³
    print(f"\nğŸ” æ­£åœ¨æå–è¯»éŸ³ä¿¡æ¯...")
    word_dict = extract_pronunciations(tagger, words)
    
    # ä¿å­˜
    save_dictionary(word_dict, output_file)
    
    # æ˜¾ç¤ºç¤ºä¾‹
    print(f"\nğŸ“ ç¤ºä¾‹æ•°æ®ï¼ˆå‰10ä¸ªï¼‰:")
    for i, (word, (pron, kana, pos, freq)) in enumerate(list(word_dict.items())[:10], 1):
        print(f"  {i:2d}. {word:15s} â†’ {pron:15s} ({pos:10s}, freq={freq})")
    
    print(f"\nâœ¨ å®Œæˆï¼")
    print(f"\nğŸ’¡ ä½¿ç”¨æ–¹æ³•:")
    print(f"   C ä»£ç å¯ä»¥åŠ è½½æ­¤æ–‡ä»¶: {output_file}")
    print(f"   æ ¼å¼: word<TAB>pronunciation<TAB>frequency<TAB>pos")
    
    return 0

if __name__ == "__main__":
    sys.exit(main())
