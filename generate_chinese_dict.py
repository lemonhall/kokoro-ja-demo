#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç”Ÿæˆä¸­æ–‡è¯å…¸ï¼ˆä» jieba æå–ï¼‰

è¾“å‡ºæ ¼å¼ï¼š
è¯<Tab>è¯é¢‘

ç›®æ ‡ï¼š10ä¸‡+ è¯æ±‡
"""

import jieba
import os
from pathlib import Path

def generate_chinese_dict():
    """ä» jieba è¯å…¸æå–ä¸­æ–‡è¯æ±‡"""
    
    # jieba é»˜è®¤è¯å…¸è·¯å¾„
    jieba.initialize()
    
    # è·å– jieba è¯å…¸æ–‡ä»¶è·¯å¾„
    jieba_dict_path = jieba.get_dict_file()
    
    print(f"ğŸ“– è¯»å– jieba è¯å…¸: {jieba_dict_path.name if hasattr(jieba_dict_path, 'name') else jieba_dict_path}")
    
    # è¯»å– jieba è¯å…¸
    words = []
    dict_file = jieba.get_dict_file()
    
    # jieba çš„è¯å…¸æ–‡ä»¶å¯èƒ½æ˜¯äºŒè¿›åˆ¶æ¨¡å¼æ‰“å¼€çš„
    if hasattr(dict_file, 'mode') and 'b' in dict_file.mode:
        # äºŒè¿›åˆ¶æ¨¡å¼
        for line in dict_file:
            line = line.decode('utf-8').strip()
            parts = line.split()
            if len(parts) >= 2:
                word = parts[0]
                try:
                    freq = int(parts[1])
                except:
                    continue
                
                # è¿‡æ»¤æ¡ä»¶ï¼š
                # 1. åªä¿ç•™ä¸­æ–‡è¯æ±‡ï¼ˆè‡³å°‘åŒ…å«ä¸€ä¸ªä¸­æ–‡å­—ç¬¦ï¼‰
                # 2. è¿‡æ»¤æ‰çº¯æ•°å­—ã€çº¯å­—æ¯
                # 3. è¯é¢‘ > 0
                if freq > 0 and any('\u4e00' <= c <= '\u9fff' for c in word):
                    words.append((word, freq))
    else:
        # æ–‡æœ¬æ¨¡å¼æˆ–è€…éœ€è¦é‡æ–°æ‰“å¼€
        dict_path = dict_file if isinstance(dict_file, str) else dict_file.name
        with open(dict_path, 'r', encoding='utf-8') as f:
            for line in f:
                parts = line.strip().split()
                if len(parts) >= 2:
                    word = parts[0]
                    try:
                        freq = int(parts[1])
                    except:
                        continue
                    
                    # è¿‡æ»¤æ¡ä»¶ï¼šåŒä¸Š
                    if freq > 0 and any('\u4e00' <= c <= '\u9fff' for c in word):
                        words.append((word, freq))
    
    # æŒ‰è¯é¢‘æ’åºï¼ˆä»é«˜åˆ°ä½ï¼‰
    words.sort(key=lambda x: x[1], reverse=True)
    
    print(f"âœ… æå–åˆ° {len(words)} ä¸ªä¸­æ–‡è¯æ±‡")
    
    # è¾“å‡ºåˆ°æ–‡ä»¶
    output_dir = Path(__file__).parent / "misaki_c_port" / "extracted_data" / "zh"
    output_dir.mkdir(parents=True, exist_ok=True)
    
    output_file = output_dir / "dict_full.txt"
    
    with open(output_file, 'w', encoding='utf-8') as f:
        for word, freq in words:
            f.write(f"{word}\t{freq}\n")
    
    print(f"ğŸ’¾ è¯å…¸å·²ä¿å­˜: {output_file}")
    print(f"ğŸ“Š è¯æ±‡æ•°é‡: {len(words)}")
    
    # ç»Ÿè®¡ä¿¡æ¯
    print("\nğŸ“Š è¯å…¸ç»Ÿè®¡:")
    print(f"  - æ€»è¯æ•°: {len(words)}")
    print(f"  - æœ€é«˜é¢‘è¯: {words[0][0]} (é¢‘ç‡: {words[0][1]})")
    print(f"  - æœ€ä½é¢‘è¯: {words[-1][0]} (é¢‘ç‡: {words[-1][1]})")
    
    # è¯é•¿åˆ†å¸ƒ
    length_dist = {}
    for word, _ in words:
        length = len(word)
        length_dist[length] = length_dist.get(length, 0) + 1
    
    print("\nğŸ“ è¯é•¿åˆ†å¸ƒ:")
    for length in sorted(length_dist.keys())[:10]:
        print(f"  - {length}å­—è¯: {length_dist[length]} ä¸ª")
    
    # ç”Ÿæˆç»Ÿè®¡ JSON
    import json
    stats_file = output_dir / "stats.json"
    with open(stats_file, 'w', encoding='utf-8') as f:
        json.dump({
            "total_words": len(words),
            "max_freq": words[0][1],
            "min_freq": words[-1][1],
            "length_distribution": length_dist,
            "top_10_words": [{"word": w, "freq": f} for w, f in words[:10]],
        }, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯å·²ä¿å­˜: {stats_file}")
    
    return output_file

if __name__ == "__main__":
    print("ğŸš€ å¼€å§‹ç”Ÿæˆä¸­æ–‡è¯å…¸...\n")
    output_file = generate_chinese_dict()
    print(f"\nâœ¨ å®Œæˆï¼è¯å…¸æ–‡ä»¶: {output_file}")
